Marco Slot worked at Citus that has been acquired by MicroSoft. He's been working for more than 10 years on Postgres. He is specialised in Distributed Systems.

Distributed Systems are complicated because there is **always some kind of trade off**. You have to give up some properties to have some others.

## Single machine PostgreSQL
PostgreSQL on a single machine can be incredibly fast because there are no network latency etc. But not used  a lot because there are a lot of failure possibles. For example if the disk fails, or there is no more space on the disk, or the computer shutdowns, we loose everything.

## PosgtreSQL distributed in the cloud
Fixing the operational hasards of  single node PostgreSQL requires a distributed set up;

The cloud enables flexible distributed set ups with resources and share between
#todo 
## Goals of distributed architecture
Goal: offer same functionality and transactional semantics as single node RDBMS with superior poroperties.

Mechanisms:
- Replication
- Distribution
- Decentralisation

In the reality, there are concessions in terms of **performance**, transactional semantics, etc.

## Different layers
In a distributed architecture, there can be multiple layers (many are orthogonal):
- Client
- Pooler
- Query engine
- Logical Data layer
- Storage manager
- Data files, WAL
- Disk

Today we will cover Network-attached cloud storage, read replicas and hot standbys, #todo 

The **two mains questions**:
- What are the trade-offs (latency, efficiency,...)?
- For which Workloads (lookups, analytical queries, small updates,...)?


Latency is a problem because transactions are synchronous and performed step by step on each session. Some commands (UPDATE for example) will **lock** the disk. You use COMMIT to be sure that the update has been done (by using the logs). This means that 1 transaction could take a while (20ms)

The number of processes on the "sever" is limited by the memory, contention, etc. So there is a maximum thoughput.
## Network-attached block storage
This method allows to store
#todo insert schema
HyperVisor pretends that there is a disk but calls APIs when needing to reach "the disk".

That means that any kind of write/reads connects to the network

This is a pretty big trade off, we gain a lot of response time.

Pros:
- Higher durability (replication)
- Higher uptime (replace VM, reattach)
- Fast backups and replica creation (snapshots)
- Disk is resizable
Cons:
- Higher disk latency (esp for writes 10 µs -> 1000 µs)
- Lower IOPS (1M -> 10k IOPS)
- Crash recovery on restart takes time
- Cost can be high
## Read replicas scaling
Another common form of distributed postgres setup.

Readable replicas can help scale read throughput, reduce latency through cross-region replication, improve availability through auto-failover.

Have one primary postgres and multiple Physical replication (data files + WAL) that will be easily access depending on the place where we make the query! (A replica per continent for example).
### Scaling read throughput
Readable replicas can help scale read throughput by load balancing queries across replicas
### Eventual read-your-writes consistency
Read replicas can be behind on the primary, cannot always read your writes.

This can happen if you write in a physical replica and the user reads in another physical replica before it has been updated. This is because the primary Postgres have not yet updated all replicas.
### No monotonic read consistency
Load-balancing across read replicas will cause you to go back-and-forth in time.
### Poor cache usage
If all replicas are equal, they all have the same stuff in cache. 

And if the working set is >>> than the memory, all replicas get bottle-necked on disk I/O.
### Pros/Cons
**Pros**:
- Read throughput scales linearly
- Low latency stale reads if read replica is closer than primary
- Lower load on primary
**Cons**:
- Eventual read-your-writes consistency
- No monotonic read consistency
- Poor cache usage

## Hot standby
#todo
Use a "private server" (hot standby) connected to "public server" (primary postgresql) that is always up to date. It allows fast recovery.

The connection between the two is optimised to have better performance and less process calculation on the hot stand.

Sometimes there can be latency (synchronous_commit = local).
### Primary failure
When a primary fails from the cluster manager point of view, it initiates failover. Make the hot standby the new primary AND demote the primary.

We need to avoid having two primary servers.

The procedure is in the slide #todo.
### Pros and Cons
**Pros**:
- Can survive node and availability zone failure
- No crash recovery when resuming from failure
**Cons**:
- #todo
## DBMS-optimised storage
Cloud storage that can perform background page writes autonomously, which saves on write I/O from primary. Also optimised for other DBMS needs.

This is done by segmenting the APIs etc. #todo add schema
## Trade offs
**Pros**:
- Potential performance benefits by avoiding page writes from primary
- No crash recovery
- Replicas can reuse storage, incl. hot standby
- Less rigid than network attached storage implementation (fast reattach, branching,...)
**Cons**:
- Write latency is high by default
- High cost / pricing
- PostgreSQL is not designed for it, can be slower than regular storage

## Shard 
### As citus


### Multi-shared query
The user does a query on the primary, and the postgres server does subqueries in other servers to increase efficiency.

It doesn't change anything for the user but it changes everything  on the server side.

The snapshot isolation is a challenge that involves trade offs.

### Trade offs
**Pros**:
- Scale throughput for read and writes (CPU and IOPS)
- Scale memory for large working sets
- Parrallelize analytical queries

**Cons**:
- High read and write latency
- Data model decisions have high impact on performance
- Snapshot isolation concessions

## Active-Active / n-way replication
Another type of architecture that has been around more and more recently.

Accepts writes from any node, use logical replication to asynchronously exchange and consolidate writes.
**Pros**:
- Very high read and write availability
- Low read and write latency
- Read throughput scales linearly
**Cons**:
- Eventual read-your-writes consistency
- No monotonic read consistency
- #todo 
## Distributed SQL
### Distributed key-value storage with SQL (DSQL)
Tables are stored on distributed key)value stores, shard replicated using Paxos/Raft.

Distributed transactions with snapshot isolation via global timestamps (HLC or TrueTime).
**Pros**:
- Good read and write availability
- Single table, single key operations scale well
- No additional
- #todo
**Cons**:
- #todo

## Conclusion
PostgreSQL can be distributed at different layers.

Each architecture can introduce severe trade-offs. Almost nothing comes for free.

Keep asking yourself:
- What do I really want ?
- Which architecture achieves that?
- What are the trade offs?
- What can my application #todo ?

Guidelines:
#todo 