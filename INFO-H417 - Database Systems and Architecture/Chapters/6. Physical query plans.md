## 6.1 Query optimisation
To know which #plan is the most efficient, we **generate and compare** all plans and **pick the plan with the minimal cost**.![](Pasted%20image%2020231226140644.png)
## 6.2 Plan generation
To **generate plans**, we use everything we know to try all possibilities:
- Transforming #relational-algebra expressions
- Use of existing #index 
- Build #index or **sort on the fly**

Some implementations are detailed in this chapter:
- Join algorithm
- Memory management
- Parallel processing
### 6.2.1 IO's and cost estimations
We **estimate** #IO's by *counting the number of disk blocks* that must be read (or written) to execute a query plan.

To **estimate** #cost, we have **additional parameters**:
- $B(R)= \#$ of blocks containing R tuples
- $f(R)=max \text{  }\#$ of tuples of R per block
- $M = \#$ memory blocks available
- $T(R)= \#$ of tuples in the relation R ($T(R)>B(R)$)
- When using #B-Tree indexes
	- $HT(i)= \#$ levels in index $i$
	- $LB(i)=\#$ of leafs blocks in index $i$
### 6.2.2 Clustering
We talk of #clustering-index from an #index that **allow tuples to be read in an order that corresponds to the physical order**.
![](Pasted%20image%2020231226142559.png)
The notion of #clustering 
- **Clustered file organisation** refers to the fact that **blocks in the file** can *store tuples from multiple relations* but they are **ordered**.![](Pasted%20image%2020231226142659.png)
- **Clustered relation** $\implies B(R) \ll T(R)$![](Pasted%20image%2020231226142709.png)
## 6.3 Join algorithms
To perform #join operations the most efficient way, different algorithms exists and are used on different cases.

Let's use an example to illustrate all those algorithms:
	Imagine we have **two relations** $R_{1}$ and $R_{2}$ that share a *common attribute* $C$. And we know that:
		- $T(R_{1})=10000$
		- $T(R_{2})=5000$
		- $S(R_{1})=S(R_{2})={1}/{10}$ block (size of the tuple)
		- Memory available $=101$ blocks
	 And we **want to perform the following transformation**: $R_{1}\bowtie R_{2}, R_{2}\bowtie R_{1}$

Now let's see how the four join algorithms will do:
- Iteration join
- Merge join
- Join with index
- Hash join
### 6.3.1 Iteration join (nested loops)
The #iteration-join uses two nested loops to compare each element (here an element is a block and not a tuple) of $R_{1}$ and $R_{2}$, it is the **intuitive way**.![](Pasted%20image%2020231226143654.png)For each possibility, we **output the pair** if it matches the #join condition.
### 6.3.2 Merge join
The #merge-join first **sorts the relations**, then a *while* loop is used to check whether both are correct, else the $i$ and $j$ are incremented depending on which one is higher/lower.
![](Pasted%20image%2020231226143950.png)
![[Pasted Image 20231226144929_958.png]]
We can stop whenever we reach the end of either $R_{1}$ or $R_{2}$.

Then the **output-tuples procedure** (here below), is the part of the algorithm that *builds the result relation*.![](Pasted%20image%2020231226144956.png)This algorithm could take a lot of time because the **sorting of big tables can take a long time**.
#### Memory requirements
The #memory needed for merge sorts depends on the number of chunks.
Let's say that we have $k$ blocks in memory and $x$ blocks for relation sort.
	$\# chunks = x/k$ and the *size of a chunk* $k$ $$\#chunks \leq \text{ buffers available for merge}$$so $x/k\leq k$ or $k\geq \sqrt{ x }$
	$\implies$ **The number of pages available in memory must be higher than the square root of the number of blocks in the relation**

So in our example, because R1 is 1000 blocks, $k \geq 31.62 \implies 32$ blocks of memory needed.  (R2 is smaller thus R1 will set the limit).

#### Improvement on Merge join
What takes a lot of time in #merge-join is the **sorting part**. But what if instead of sorting the entire files, we **just compare the heads** while sorting. If one is lower than the others, it won't be joined anymore so we can read the next block.

In terms of #cost ![](Pasted%20image%2020231226184411.png)
And in term of #memory requirements, we need one memory buffer per sorted runs to apply the merge join.
#### Example
![](Pasted%20image%2020231226144035.png)
#todo sample run

### 6.3.4 Iteration vs Merge join
##### Not contiguous Join R1 $\bowtie$ R2
The fact that it is **not contiguous** is the **worst case** that could happen because when we read the next tuple, we have to bring the next page into the memory. So we have to **count 1** #IO **per read** (as if we always needed the next page)!
1. #iteration-join 
	For each tuple of $R_{1} \to 10.000[1+5000]=50.010.000$ #IO's
	This is **way too much**, so let's use our memory to improve it.
	1. Read 100 blocks of $R_{1}$
	2. Read all of $R_{2}$ (using 1 block) + join
	3. Repeat until done
	$\implies$ for each R1 chunk, read chunk costs $1000$ IOs and read R2 5000 IOs $\implies 6000$ #IO's

	So this leaves us with the total of $10.000/1000$ chunks of R1 that each have $6000$ IOs.
	$R_{1}\bowtie R_{2} = \frac{10000}{1000}\times(1000+5000)=60.000$
	
	It is **way better** to use the memory.

	To improve, the reverse method will be more effective as there are less tuples in R2.
	$R_{2}\bowtie R_{1} = \frac{5000}{1000}\times(1000+10000)=55.000$
2. #merge-join requires sequential + ordered
	- **Sequentiality**: (read is not sequential and write is)
		read R1 + write R1 = 10.000 + 1000 = 11.000
		read R2 + write R2 = 5000 + 500 = 5.500
		$\implies 16.000$		
	- **Order**: (already sequential)
		 $2 \times$ (read R1+ write R1) $= 2\times 2000 = 4000$
		 $2 \times$ (read R2+ write R2) $= 2\times 1000 = 2000$
		 $\implies 6.000$
	- **Join**: read R1 + read R2 $= 1000+500=1.500$
	$\implies 23.500$
##### Contiguous
1.  #iteration-join 
	 $R_{1}\bowtie R_{2} \implies \frac{10.000}{1000}\times(100+500)=6.000$
	 $R_{2}\bowtie R_{1} \implies \frac{5.000}{1.000}\times(100+1000)=5.500$
2. #merge-join 
	- **Order**: 
		 $2\times(1000+1000)=4000$
		 $2\times(500+500)=2000$
		 $\implies 6000$
	- **Join**:
		 read R1 + read R2 $=1500$
	$\implies 7500$
##### Contiguous + ordered
#iteration-join $\implies 5.500$ nothing changes
#merge-join $\implies 1.500$ because no need to sort!
##### Contiguous and unordered
#iteration-join 
Total cost = $\frac{B(R_{1})}{M-1}(M-1 + B(R_{1}))$

#merge-join 
- Sort R1: $4 B(R_{1})$
- Sort R2: $4 B(R_{2})$
- Merge: $B(R_{1})+B(R_{2})$
$\implies 5\times(B(R_{1})+B(R_{2}))$

To **determine whether to use iteration or merge** algorithm, let's see the break point between both.![](Pasted%20image%2020231226182011.png)
$\implies B(R_{1}) = 5 \frac{B(R_{2})}{\frac{B(R_{2})}{M-1}-4}$

So here, in our example, knowing that $M=101$ and $B(R_{2})=5.000$
The break point is at $\frac{25000}{\frac{5000}{100}-4}=\frac{25000}{46}=544 = B(R_{1})$

We use #iteration-join > #merge-join if $\implies B(R_{1}) < 5 \frac{B(R_{2})}{\frac{B(R_{2})}{M-1}-4}$
### 6.3.3 Join with index
The notation $$X \leftarrow index(rel,attr,value)$$ says that $X$ is the **set of relation tuples with the value at the attribute**.

For this one, we **assume that** $R_{2}.C$ **has an index**.
![](Pasted%20image%2020231226170512.png)
We retrieve the tuples of $R_{1}$ with the same value as $r.C$ in $C$.

 Reads 500 IOs *5000 tuples in blocks of 10 tuples $\to$ 500 blocks*
	 For each R2 tuple
		 probe index - free
		 if match, read R1 tuple: 1 IO *output of a pair require to read a block of R1*
	Now we need to know how much tuples will match
		1.  say R1.C is a key, R2.C is a foreign key then $\implies 1$
			*Maximum one matching value since keys are unique*
		2. Say V(R1, C) = 5000, T(R1) = 10.000 with uniform assumption $\implies \frac{10.000}{5.000}=2$
		3. Say Domain of C/ DOM(R1, C) = 1.000.000 and T(R1)=10.000 with alternate assumption $\implies \frac{10.000}{1.000.000}=\frac{1}{100}\%$ of matching tuples
#### Example
Let's assume that
- R1.C index exists and has 2 levels
- R2 is **contiguous** and **unordered**
- R1.C index fits in memory
	 This means that querying the index is free, read all tuples of R2 will be the only cost!
	 Note that this assumption is not always (and not often) valid.

The #cost 
	1. Total cost = 500+5000(1)1 = 5500 *R.C are keys*
	2. Total cost = 500+5000(2)1 = 10500 *uniform assumption*
	3. Total cost = 500 + 5000(1/100)1 = 550 *alternate assumption*
	$\implies$ requires one more parameter, the #selectivity

Say that the **index does not fit in memory**. For  example say R1.C index is 201 blocks.
- Keep root + 99 leaf nodes in memory
- Expected cost of each probe is $E = \frac{(0)99}{200} + \frac{(1)101}{200}$
	 The 0 is the cost if in memory (0 because access in memory is free)
	 The 1 is the cost of bringing a page to the memory

The **total cost** including probes
2. $=500+5000 [0.5+2]=500+12.500=13.000$ 
3. $=500+5000[0.5\times {1}+ (1/100)\times {1}]=500+2500+50=3050$ IOs
### 6.3.4 Hash join
- Hash function $h$, range $0\to k$
- Buckets for $R_{1}:G_{0},G_{1},\dots,G_{k}$
- Buckets for $R_{2}:H_{0},H_{1},\dots,H_{k}$

The algorithm works the following way:
1. Hash $R_{1}$ tuples into $G$ buckets
2. Hash $R_{2}$ tuples into $H$ buckets
3. For i = 0 to k do
		match tuples in $G_{i}, H_{i}$ buckets

It is **faster** because here we only compare bucket by buckets. Comparing $G_{1}$ to $H_{2}$ has no sense because it will **never** give a match.![](Pasted%20image%2020231226171549.png)
#### Memory requirements
The size of R1 bucket = x/k
- k = number of memory buffers
- x = number of R1 blocks

So x/k < k $\implies k > \sqrt{ x }$, we need $k+1$ buffers.
#### Example
Let's say that R1 and R2 are **contiguous** and **unordered**, that we use 100 buckets and read R1, hash + write buckets.
	All tuples in one bucket have the same hash.
	In average, 10 blocks (not guaranteed), if the hash function is really good, it will be evenly distributed.

The same is done for R2.

Then we read one R1 bucket, build a memory hash table and read the corresponding R2 bucket + hash probe.

Repeat for all buckets.

In terms of #cost:
* **Bucketize**: Read R1 + write, Read R2 + write 
* **Join**: Read R1, R2
$\implies Total$ $Cost = 3\times[1000+500]=4500$
But this is an approximation since buckets will vary in size and we have to round up to blocks.
